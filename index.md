**弱监督语义分割：**

1.[iccv 2025]Class Token as Proxy: Optimal Transport-assisted Proxy Learning for Weakly
Supervised Semantic Segmentation[[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Class_Token_as_Proxy_Optimal_Transport-assisted_Proxy_Learning_for_Weakly_ICCV_2025_paper.pdf)

2.[iccv 2025]Know Your Attention Maps: Class-specific Token Masking for
Weakly Supervised Semantic Segmentation[[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Hanna_Know_Your_Attention_Maps_Class-specific_Token_Masking_for_Weakly_Supervised_ICCV_2025_paper.pdf)[[code]](https://github.com/HSG-AIML/TokenMasking-WSSS)

3.[iccv 2025]Bias-Resilient Weakly Supervised Semantic Segmentation Using Normalizing
Flows[[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Qiu_Bias-Resilient_Weakly_Supervised_Semantic_Segmentation_Using_Normalizing_Flows_ICCV_2025_paper.pdf)

4.[cvpr 2025]Exploring CLIP’s Dense Knowledge for Weakly Supervised Semantic
Segmentation[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Exploring_CLIPs_Dense_Knowledge_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)[[code]](https://github.com/zwyang6/ExCEL)


5.[cvpr 2025]FFR: Frequency Feature Rectification for Weakly Supervised Semantic
Segmentation[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_FFR_Frequency_Feature_Rectification_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)

6.[cvpr 2025]POT: Prototypical Optimal Transport for Weakly Supervised Semantic
Segmentation[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_POT_Prototypical_Optimal_Transport_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)

7.[cvpr 2025]Multi-Label Prototype Visual Spatial Search for Weakly Supervised
Semantic Segmentation[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.pdf)

8.[cvpr 2025]Weakly Supervised Semantic Segmentation via Progressive Confidence Region
Expansion[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Xu_Weakly_Supervised_Semantic_Segmentation_via_Progressive_Confidence_Region_Expansion_CVPR_2025_paper.pdf)

9.[cvpr 2024]Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Frozen_CLIP_A_Strong_Backbone_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2024_paper.pdf)[[code]](https://github.com/zbf1991/WeCLIP)

10.[TPAMI2025]Frozen CLIP-DINO: a Strong Backbone for Weakly Supervised Semantic Segmentation[[paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10891864)

11.[AAAI 2025]Toward Modality Gap: Vision Prototype Learning for Weakly-supervised Semantic Segmentation with CLIP[[paper]](https://arxiv.org/pdf/2412.19650)

12.[ECCV 2024]DIAL: Dense Image-text ALignment for Weakly Supervised Semantic Segmentation[[paper]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08737.pdf)







**语义分割：**

1.[iccv 2025]FLOSS:FreeLunchinOpen-vocabularySemanticSegmentation[[paper]](https://arxiv.org/pdf/2504.10487)

2.[iccv 2025]DenseVLM: A Retrieval and Decoupled Alignment Framework
for Open-Vocabulary Dense Prediction[[paper]](https://arxiv.org/pdf/2412.06244)

3.[iccv 2025]Feature Purification Matters: Suppressing Outlier Propagation for
Training-Free Open-Vocabulary Semantic Segmentation[[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Jin_Feature_Purification_Matters_Suppressing_Outlier_Propagation_for_Training-Free_Open-Vocabulary_Semantic_ICCV_2025_paper.pdf)

4.[iccv 2025]Training-Free Class Purification for Open-Vocabulary Semantic Segmentation[[paper]](https://openaccess.thecvf.com/content/ICCV2025/papers/Chen_Training-Free_Class_Purification_for_Open-Vocabulary_Semantic_Segmentation_ICCV_2025_paper.pdf)



# Here are some papers on  Visual Place Recognition:
**老师提供的写作参考：**

1. A HARD-TO-BEAT BASELINE FOR TRAINING-FREE CLIP-BASED ADAPTATION[[paper]](https://arxiv.org/pdf/2402.04087)

2. Discriminant analysis by Gaussian mixtures[[paper]](https://watermark02.silverchair.com/jrsssb_58_1_155.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA2kwggNlBgkqhkiG9w0BBwagggNWMIIDUgIBADCCA0sGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMle_7yHlRxGc6ithuAgEQgIIDHP0ZFQa0u7DsAJSHHy3cnKtnTniuXae5NlIFUX7pxppfBhXVuXJHr-cqhtr8zizGlNGb5sZRpU5vZz5YDuxq5qmEWib2mZ33tl2Z3s_2FEtHsbRko0di7_XNSVJxr-WvflZm3TofmiUuBJKsWarcZAWuPf35eBAy0pbb2Cmu0pneeK5LSQHI2D6DXZdH-V2fQ0O5yjwJaRPlsNFLvRy7D_U0SoiAN7eyInB6HSkEawi22BrY_AQQmnJa_I_JH44M35GXR3z1pq9DQ2krKXU2XLcasFPGQePBoHHwhbU5rMBGkg2I1eYKEk6-YBjEqdyXVLmT_Y4L1us_FCw00lYS7ulIGYdM9BgJpd-31F4necLMvNCYa8H1L4xJvjMoF2ZkZO00gyoQbgWs-MuY0fh5WEBeTwANK4dLUfR3OPimlPQjcBcRQv6KqfD32io1LMkkUEt5FxHOoN7inBBQbBn04k7F41yqgcfzTrB6KFnQWq43Is6MeeyMoujbMVZ93KCuz5CWmDHUB3tDd5erBZHzD1vntoJO6rcK3JxAG028dNcSQl0chZQgDeCTAAEAX1wdjbhJXsZJWMjhjEzhud0h4f4Mf8_15B5Gql-RQhYNj8xLQebcsgBf2YNJI0gTrdiUpNOCe0xu52X4Z1A8vziIMup79b6iUAavWaBsRpPZD7p2njakmR3gfY4hbi6EhgU5AJACl8Tc46hQXYHH8gqglPYp4qkmZMN8QKoCnPxYA3edIOTPt6diKAkZawMLhRSe_KQQdt5_PaQejJ3rYctOhVD-bfXp1LZwlotPgpnIOV7XykXqPEbcHuFN2vLPy1DBFhquq_10gi5FlRwhusgnLip54rJs3i5KMxxRK9acau_9A-LXbZxC5jXYaDJpftwbXzlGU37ezv4pNpX38M5Rn1GWuaj6NZELynWt5R_gAG6x7PiaiV3Aevs3o_DF-9vZNntUAFIF84MiOysr6spbr4Tn5dQStzKRoWYHLZV1H90XY6LRO3wDr7DYWNsjXQmd8TE22QSr8NW3NqgxQixG7MPQI6V3TVMk9vm6MnM)

3. Disentangling CLIP for Multi-Object Perception[[paper]](https://arxiv.org/pdf/2502.02977v3)

4. Talking to DINO: Bridging Self-Supervised Vision Backbones with Language
for Open-Vocabulary Segmentation[[paper]](https://arxiv.org/pdf/2411.19331)

5. [under review ICLR 2026]DE-HALLUCINATING CLIP EMBEDDINGS TO IMPROVE
BRAIN-VISION MAPPING[[paper]](https://openreview.net/attachment?id=oKHPJ0GTLG&name=pdf)
6. Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian Alignment[[paper]](https://arxiv.org/pdf/2508.15568)

**可能对多标签图像分类有用：**
1. [CVPR 2025]Prompt-Guided Attention Head Selection for Focus-Oriented Image Retrieval[[paper]](https://arxiv.org/pdf/2504.01348)
2. [preprint]ONE PATCH TO CAPTION THEM ALL: A UNIFIED ZEROSHOT CAPTIONING FRAMEWORK[[paper]](https://arxiv.org/pdf/2510.02898)[[code]](https://github.com/Ruggero1912/Patch-ioner)

**多标签图像分类：**
1. [ICCV 2023]PatchCT: Aligning Patch Set and Label Set with Conditional Transport
for Multi-Label Image Classification[[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_PatchCT_Aligning_Patch_Set_and_Label_Set_with_Conditional_Transport_ICCV_2023_paper.pdf)
2. [ICCV 2023]Cdul: Clip-driven unsupervised learning for multi-label image classification[[paper]](https://arxiv.org/pdf/2307.16634)[[code]](https://github.com/cs-mshah/CDUL)
3. [ICML 2024]Language-driven Cross-modal Classifier for
Zero-shot Multi-label Image Recognition[[paper]](https://openreview.net/pdf?id=sHswzNWUW2)[[code]](https://github.com/yic20/CoMC)
4. [AAAI 2024]TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training[[paper]](https://arxiv.org/pdf/2312.12828)[[code]](https://github.com/linyq2117/TagCLIP)
5. [CVPR 2025]SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models[[paper]](https://arxiv.org/pdf/2502.16911?)[[code]](https://github.com/kjmillerCURIS/SPARC)
6. [CVPR 2025]Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification[[paper]](https://arxiv.org/pdf/2503.16873)[[code]](https://github.com/k0u-id/CCD)
7. [CVPR 2025]Recover and Match: Open-Vocabulary Multi-Label Recognition through
Knowledge-Constrained Optimal Transport[[paper]](https://arxiv.org/pdf/2503.15337)[[code]](https://github.com/EricTan7/RAM)
8. [CVPR 2025]Correlative and Discriminative Label Grouping for Multi-Label
Visual Prompt Tuning[[paper]](https://arxiv.org/pdf/2504.09990)
9. [ICML 2025]From Local Details to Global Context:Advancing Vision-Language Models with Attention-Based Selection[[paper]](https://arxiv.org/pdf/2505.13233?)[[code]](https://github.com/BIT-DA/ABS)
10. [WACV 2025]Pay Attention to Your Neighbours:Training-Free Open-Vocabulary Semantic Segmentation[[paper]](https://arxiv.org/pdf/2404.08181?)[[code]](https://github.com/sinahmr/NACLIP)
11. [ICCV 2025]Category-Specific Selective Feature Enhancement for Long-Tailed Multi-Label Image Classification
12. [unknown]Self-Calibrated CLIP for Training-Free Open-Vocabulary Segmentation[[paper]](https://arxiv.org/pdf/2411.15869)[[code]](https://github.com/SuleBai/SC-CLIP)
13. [ICCV 2025]MambaML: Exploring State Space Models for Multi-Label Image Classification[[paper]](https://papers.cool/venue/Zhu_MambaML_Exploring_State_Space_Models_for_Multi-Label_Image_Classification@ICCV2025@CVF)
14. [ICCV 2025]Category-Specific Selective Feature Enhancement for Long-Tailed Multi-Label Image Classification
15. [ICCV 2025]More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning[[paper]](https://arxiv.org/pdf/2508.20381v1)
16. [ICCV 2025]Language-Driven Multi-Label Zero-Shot Learning with Semantic Granularity
17. [under review ICLR 2026]Unlocking the Power of Co-Occurrence in CLIP: A DualPrompt-Driven Method for Training-Free Zero-Shot Multi-Label Classification[[paper]](https://openreview.net/forum?id=QGXVZ0OPLy)
18. [under review ICLR 2026]EFFICIENTLY DISENTANGLING CLIP FOR MULTIOBJECT PERCEPTION[[paper]](https://openreview.net/attachment?id=7BZuePfFci&name=pdf)

**转导学习**

1.[neurips 2024]Boosting Vision-Language Models with Transduction[[paper]](https://arxiv.org/pdf/2406.01837)[[code]](https://github.com/MaxZanella/transduction-for-vlms)


**One Stage VPR:**
1. [ICCV 2025]Cloak: A First Look at Privacy Cloak Against Visual Place Recognition[[paper]]
2. [ICCV 2025]A Hyperdimensional One Place Signature to Represent Them All: Stackable Descriptors For Visual Place Recognition[[paper]](https://arxiv.org/pdf/2412.06153)[[code]](https://github.com/CMalone-Jupiter/HOPS)
3. [WACV 2025]Breaking the Frame:
Visual Place Recognition by Overlap Prediction[[paper]](https://openaccess.thecvf.com/content/WACV2025/papers/Wei_Breaking_the_Frame_Visual_Place_Recognition_by_Overlap_Prediction_WACV_2025_paper.pdf)[[code]](https://github.com/weitong8591/vop)
4. [ICLR 2025]EFFOVPR: EFFECTIVE FOUNDATION MODEL UTILIZATION FOR VISUAL PLACE RECOGNITION[[paper]](https://openreview.net/forum?id=NSpe8QgsCB)
5. [ECCV 2024]Close, But Not There: Boosting Geographic Distance Sensitivity in Visual Place Recognition[[paper]](https://arxiv.org/pdf/2407.02422)[[code]](https://github.com/serizba/cliquemining)
6. [CVPR 2024]EarthLoc: Astronaut Photography Localization by Indexing Earth from Space[[paper]](https://arxiv.org/pdf/2403.06758)[[code]](https://github.com/gmberton/EarthLoc)
7. [ECCV 2024]VLAD-BuFF: Burst-aware Fast Feature Aggregation for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2409.19293)[[code]](https://github.com/Ahmedest61/VLAD-BuFF/)
8. [CVPR 2024]CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2402.19231)[[code]](https://github.com/Lu-Feng/CricaVPR)
9. [ECCV 2024][Statewide Visual Geolocalization in the Wild][[paper]](https://arxiv.org/pdf/2409.16763)[[code]](https://github.com/fferflo/statewide-visual-geolocalization)
10. [CVPR 2024]BoQ: A Place is Worth a Bag of Learnable Queries[[paper]](https://arxiv.org/pdf/2405.07364)[[code]](https://github.com/amaralibey/Bag-of-Queries)
11. [CVPR 2024]Optimal Transport Aggregation for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2311.15937)[[code]](https://github.com/serizba/salad)
12. [ICCV 2023]EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2308.10832)[[code]](https://github.com/gmberton/EigenPlaces)
13. [cvpr 2023]Data-efficient Large Scale Place Recognition with Graded Similarity Supervision[[paper]](https://arxiv.org/pdf/2303.11739)
14. [preprint]HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular
Visual Place Recognition[[paper]](https://arxiv.org/pdf/2506.04764)[[code]](https://github.com/suhan-woo/HypeVPR)
14.SelaVPR++: Towards Seamless Adaptation of Foundation Models for Efficient Place Recognition[[paper]](https://arxiv.org/pdf/2502.16601)[[code]](https://github.com/Lu-Feng/SelaVPR)
15. [WACV 2023]MixVPR: Feature Mixing for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2303.02190)[[code]](https://github.com/amaralibey/MixVPR)
16. [CVPR 2022]Rethinking Visual Geo-localization for Large-Scale Applications[[paper]](https://arxiv.org/pdf/2204.02287)[[code]](https://github.com/gmberton/CosPlace)

   
   
**Two Stage VPR:**
1. [AAAI 2024]Deep Homography Estimation for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2402.16086)[[code]](https://github.com/Lu-Feng/DHE-VPR)
2. [ICLR 2024]TOWARDS SEAMLESS ADAPTATION OF PRE-TRAINED MODELS FOR VISUAL PLACE RECOGNITION[[paper]](https://arxiv.org/pdf/2402.14505)[[code]](https://github.com/Lu-Feng/SelaVPR)
   
**Dimensionality Reduction：**
1. [ICML 2021]S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning[[paper]](https://arxiv.org/pdf/2009.08348)[[code]](https://github.com/MLforHealth/S2SD)
2. [CVPR 2024]VkD : Improving Knowledge Distillation using Orthogonal Projections[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Miles_VkD_Improving_Knowledge_Distillation_using_Orthogonal_Projections_CVPR_2024_paper.pdf)
   
**Others:**
1. [ECCV 2024]AddressCLIP: Empowering Vision-Language Models for City-wide Image Address Localization[[paper]](https://arxiv.org/pdf/2407.08156)[[code]](https://github.com/xsx1001/AddressCLIP)
2. [ECCV 2024]Revisit Anything: Visual Place Recognition via Image Segment Retrieval[[paper]](https://arxiv.org/pdf/2409.18049)[[code]](https://github.com/AnyLoc/Revisit-Anything)
3. [ICANN 2024]ProGEO: Generating Prompts through Image-Text Contrastive Learning for Visual Geo-localization[[paper]](https://arxiv.org/pdf/2406.01906)[[code]](https://github.com/Chain-Mao/ProGEO)
4. [SISAP 2024]Optimizing CLIP Models for Image Retrieval with Maintained Joint-Embedding Alignment[[paper]](https://arxiv.org/pdf/2409.01936)[[code]](https://github.com/Visual-Computing/MCIP)
5. [ECCV 2024]Open-World Dynamic Prompt and Continual Visual Representation Learning[[paper]](https://www.arxiv.org/pdf/2409.05312)
6. [AAAI 2024]Game4Loc: A UAV Geo-Localization Benchmark from Game Data[[paper]](https://arxiv.org/pdf/2409.16925)[[code]](https://yux1angji.github.io/game4loc)
7. [UNKNOWN]Efficient and Discriminative Image Feature Extraction for Universal Image Retrieval[[paper]](https://arxiv.org/pdf/2409.13513)[[code]](https://github.com/morrisfl/UniFEx)
8. [NeurIPS 2024 ]EMVP: Embracing Visual Foundation Model for Visual Place Recognition with Centroid-Free Probing[[paper]](https://openreview.net/pdf?id=V6w7keoTqn)[[code]](https://github.com/vincentqqb/EMVP)
9. [NeurIPS 2024 ]SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition[[paper]](https://openreview.net/pdf?id=bZpZMdY1sj)[[code]](https://github.com/lu-feng/SuperVLAD)
   


**图像匹配：**
1. [CVPR 2020]SuperGlue: Learning Feature Matching with Graph Neural Networks
[[paper]](https://arxiv.org/pdf/1911.11763v2)[[code]](https://github.com/magicleap/SuperGluePretrainedNetwork)
2. [CVPR 2021]LoFTR: Detector-Free Local Feature Matching with Transformers[[paper]](https://arxiv.org/pdf/2104.00680)[[code]](https://zju3dv.github.io/loftr/)
3. [ECCV 2022]ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer[[paper]](https://arxiv.org/pdf/2208.14201)
4. [CVPR 2023]SFD2: Semantic-guided Feature Detection and Description[[paper]](https://arxiv.org/pdf/2304.14845)[[code]](https://github.com/feixue94/sfd2)
5. [ICCV 2023]LightGlue: Local Feature Matching at Light Speed[[paper]](https://arxiv.org/pdf/2405.12979)[[code]](https://github.com/cvg/LightGlue)
6. [CVPR 2024]OmniGlue: Generalizable Feature Matching with Foundation Model Guidance[[paper]](https://arxiv.org/pdf/2405.12979)[[code]](https://hwjiang1510.github.io/OmniGlue/)
7. [CVPR 2024]XFeat: Accelerated Features for Lightweight Image Matching[[paper]](https://arxiv.org/pdf/2404.19174)[[code]](https://www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24/)

**news：**
1. [CVPR 2024]Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning[[paper]](https://arxiv.org/pdf/2404.07713)[[code]](https://github.com/shiming-chen/ZSLViT)


**无人机benchmark**

1.[CVPR 2025]Exploring the best way for UAV visual localization under Low-altitude
Multi-view Observation Condition: a Benchmark[[paper]](https://arxiv.org/pdf/2503.10692)[[code]](https://github.com/UAV-AVL/Benchmark)

2.[CVPR 2025]Multi-Modal Aerial-Ground Cross-View Place Recognition with Neural ODEs[[paper]](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Multi-Modal_Aerial-Ground_Cross-View_Place_Recognition_with_Neural_ODEs_CVPR_2025_paper.pdf)

3.[ICCV 2025]TopicGeo: An Efficient Unified Framework for Geolocation

4.[ICCV 2025]MMGeo: Multimodal Compositional Geo-Localization for UAVs

5.[Preprint]WeatherPrompt: Multi-modality Representation
Learning for All-Weather Drone Visual
Geo-Localization[[paper]](https://www.arxiv.org/pdf/2508.09560)

   

      

      

---

