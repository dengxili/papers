# Here are some papers on  Visual Place Recognition:

**One Stage VPR:**
1. [CVPR 2024]BoQ: A Place is Worth a Bag of Learnable Queries[paper](https://arxiv.org/pdf/2405.07364)
2. [ECCV 2024]Close, But Not There: Boosting Geographic Distance Sensitivity in Visual Place Recognition[paper](https://arxiv.org/pdf/2407.02422)
3. [CVPR 2024][EarthLoc: Astronaut Photography Localization by Indexing Earth from Space](https://arxiv.org/pdf/2403.06758)
4. [ECCV 2024][VLAD-BuFF: Burst-aware Fast Feature Aggregation for Visual Place Recognition](https://arxiv.org/pdf/2409.19293)
5. [ICCV 2023][EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition](https://arxiv.org/pdf/2308.10832)
6. [CVPR 2024][CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition](https://arxiv.org/pdf/2402.19231)
7. [ECCV 2024][Statewide Visual Geolocalization in the Wild](https://arxiv.org/pdf/2409.16763)
8. [ICML 2021][S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning](https://arxiv.org/pdf/2009.08348)
9. [ICLR 2025][EFFOVPR: EFFECTIVE FOUNDATION MODEL UTILIZATION FOR VISUAL PLACE RECOGNITION](https://openreview.net/forum?id=NSpe8QgsCB)
10. [CVPR 2024][Optimal Transport Aggregation for Visual Place Recognition](https://arxiv.org/pdf/2311.15937)
   
**Supplement to the one-stage method：**
1. [WACV 2023][MixVPR: Feature Mixing for Visual Place Recognition](https://arxiv.org/pdf/2303.02190)
2. [CVPR 2022][Rethinking Visual Geo-localization for Large-Scale Applications](https://arxiv.org/pdf/2204.02287)
   
**Two Stage VPR:**
1. [AAAI 2024][Deep Homography Estimation for Visual Place Recognition](https://arxiv.org/pdf/2402.16086)
2. [ICLR 2024][TOWARDS SEAMLESS ADAPTATION OF PRE-TRAINED MODELS FOR VISUAL PLACE RECOGNITION](https://arxiv.org/pdf/2402.14505)
   
**Dimensionality Reduction：**
1. [ICML 2021][S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning](https://arxiv.org/pdf/2009.08348)
2. [CVPR 2024][CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition](https://arxiv.org/pdf/2402.19231)
   
**Others:**
1. [ECCV 2024][AddressCLIP: Empowering Vision-Language Models for City-wide Image Address Localization](https://arxiv.org/pdf/2407.08156)
2. [ECCV 2024][Revisit Anything: Visual Place Recognition via Image Segment Retrieval](https://arxiv.org/pdf/2409.18049)
3. [ICANN 2024][ProGEO: Generating Prompts through Image-Text Contrastive Learning for Visual Geo-localization](https://arxiv.org/pdf/2406.01906)
4. [SISAP 2024][Optimizing CLIP Models for Image Retrieval with Maintained Joint-Embedding Alignment](https://arxiv.org/pdf/2409.01936)
5. [ECCV 2024][Open-World Dynamic Prompt and Continual Visual Representation Learning](https://www.arxiv.org/pdf/2409.05312)
6. [AAAI 2024][Game4Loc: A UAV Geo-Localization Benchmark from Game Data](https://arxiv.org/pdf/2409.16925)
7. [UNKNOWN][Efficient and Discriminative Image Feature Extraction for Universal Image Retrieval](https://arxiv.org/pdf/2409.13513)
8. [NeurIPS 2024 ][EMVP: Embracing Visual Foundation Model for Visual Place Recognition with Centroid-Free Probing](https://openreview.net/pdf?id=V6w7keoTqn)
9. [NeurIPS 2024 ][SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition](https://openreview.net/pdf?id=bZpZMdY1sj)
   
 **just change loss funtion:**  
1.[ICLR 2025][CLASS-RELATIONAL LABEL SMOOTHING FOR LIFELONG VISUAL PLACE RECOGNITION](https://openreview.net/pdf?id=ZS1lCBLljq)

**一个视觉场景的表征可以看成多个Part的聚合:**  
1.[TPAMI 2024][Unsupervised Part Discovery via Dual Representation Alignment](https://arxiv.org/abs/2408.08108)

**在测试时对特征进行矫正/About test time:**  
1. [ICLR 2025][TEST-TIME ADAPTATION FOR CROSS-MODAL RETRIEVAL WITH QUERY SHIFT](https://arxiv.org/pdf/2410.15624)
2. [NeurIPS 2024][BendVLM: Test-Time Debiasing of Vision-Language Embeddings](https://arxiv.org/pdf/2411.04420) 
3. [NeurIPS 2024][L-TTA: Lightweight Test-Time Adaptation Using a Versatile Stem Layer](https://openreview.net/pdf/f00f5429bf30e23d67511a8233740cf63a50c6e7.pdf) 
4. [NeurIPS 2024][BoostAdapter: Improving Vision-Language Test-Time Adaptation via Regional Bootstrapping](https://arxiv.org/pdf/2410.15430v2) 
5. [NeurIPS 2024][Frustratingly Easy Test-Time Adaptation of Vision-Language Models](https://arxiv.org/pdf/2405.18330) 
6. [NeurIPS 2024][Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach](https://arxiv.org/pdf/2408.07511) 
7. [NeurIPS 2024][Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models](https://arxiv.org/pdf/2410.12790) 
8. [ICLR 2025][TEST-TIME ENSEMBLE VIA LINEAR MODE CONNECTIVITY: A PATH TO BETTER ADAPTATION](https://openreview.net/pdf?id=4wk2eOKGvh) 
9. [CVPR 2024][Efficient Test-Time Adaptation of Vision-Language Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Karmanov_Efficient_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2024_paper.pdf)

**细粒度识别：**
1. [ICCV 2023][PDiscoNet: Semantically consistent part discovery for fine-grained recognition](https://openaccess.thecvf.com/content/ICCV2023/papers/van_der_Klis_PDiscoNet_Semantically_consistent_part_discovery_for_fine-grained_recognition_ICCV_2023_paper.pdf)
2. [ECCV 2024][PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers](https://arxiv.org/abs/2407.04538)


**监督对比学习：**
1. [NeurIPS-2020][Supervised Contrastive Learning](https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf)
   

      

      

---
**更新日期：2024年12月13**
