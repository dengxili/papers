# Here are some papers on  Visual Place Recognition:

**One Stage VPR:**
1. [ICLR 2025]EFFOVPR: EFFECTIVE FOUNDATION MODEL UTILIZATION FOR VISUAL PLACE RECOGNITION[[paper]](https://openreview.net/forum?id=NSpe8QgsCB)
2. [ECCV 2024]Close, But Not There: Boosting Geographic Distance Sensitivity in Visual Place Recognition[[paper]](https://arxiv.org/pdf/2407.02422)[[code]](https://github.com/serizba/cliquemining)
3. [CVPR 2024]EarthLoc: Astronaut Photography Localization by Indexing Earth from Space[[paper]](https://arxiv.org/pdf/2403.06758)[[code]](https://github.com/gmberton/EarthLoc)
4. [ECCV 2024]VLAD-BuFF: Burst-aware Fast Feature Aggregation for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2409.19293)[[code]](https://github.com/Ahmedest61/VLAD-BuFF/)
5. [CVPR 2024]CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2402.19231)[[code]](https://github.com/Lu-Feng/CricaVPR)
6. [ECCV 2024][Statewide Visual Geolocalization in the Wild][[paper]](https://arxiv.org/pdf/2409.16763)[[code]](https://github.com/fferflo/statewide-visual-geolocalization)
7. [CVPR 2024]BoQ: A Place is Worth a Bag of Learnable Queries[[paper]](https://arxiv.org/pdf/2405.07364)[[code]](https://github.com/amaralibey/Bag-of-Queries)
8. [CVPR 2024]Optimal Transport Aggregation for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2311.15937)[[code]](https://github.com/serizba/salad)
9. [ICCV 2023]EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2308.10832)[[code]](https://github.com/gmberton/EigenPlaces)

   
**Supplement to the one-stage method：**
1. [WACV 2023]MixVPR: Feature Mixing for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2303.02190)[[code]](https://github.com/amaralibey/MixVPR)
2. [CVPR 2022]Rethinking Visual Geo-localization for Large-Scale Applications[[paper]](https://arxiv.org/pdf/2204.02287)[[code]](https://github.com/gmberton/CosPlace)
   
   
**Two Stage VPR:**
1. [AAAI 2024]Deep Homography Estimation for Visual Place Recognition[[paper]](https://arxiv.org/pdf/2402.16086)[[code]](https://github.com/Lu-Feng/DHE-VPR)
2. [ICLR 2024]TOWARDS SEAMLESS ADAPTATION OF PRE-TRAINED MODELS FOR VISUAL PLACE RECOGNITION[[paper]](https://arxiv.org/pdf/2402.14505)[[code]](https://github.com/Lu-Feng/SelaVPR)
   
**Dimensionality Reduction：**
1. [ICML 2021]S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning[[paper]](https://arxiv.org/pdf/2009.08348)[[code]](https://github.com/MLforHealth/S2SD)
   
**Others:**
1. [ECCV 2024]AddressCLIP: Empowering Vision-Language Models for City-wide Image Address Localization[[paper]](https://arxiv.org/pdf/2407.08156)[[code]](https://github.com/xsx1001/AddressCLIP)
2. [ECCV 2024]Revisit Anything: Visual Place Recognition via Image Segment Retrieval[[paper]](https://arxiv.org/pdf/2409.18049)[[code]](https://github.com/AnyLoc/Revisit-Anything)
3. [ICANN 2024]ProGEO: Generating Prompts through Image-Text Contrastive Learning for Visual Geo-localization[[paper]](https://arxiv.org/pdf/2406.01906)[[code]](https://github.com/Chain-Mao/ProGEO)
4. [SISAP 2024]Optimizing CLIP Models for Image Retrieval with Maintained Joint-Embedding Alignment[[paper]](https://arxiv.org/pdf/2409.01936)[[code]](https://github.com/Visual-Computing/MCIP)
5. [ECCV 2024]Open-World Dynamic Prompt and Continual Visual Representation Learning[[paper]](https://www.arxiv.org/pdf/2409.05312)
6. [AAAI 2024]Game4Loc: A UAV Geo-Localization Benchmark from Game Data[[paper]](https://arxiv.org/pdf/2409.16925)[[code]](https://yux1angji.github.io/game4loc)
7. [UNKNOWN]Efficient and Discriminative Image Feature Extraction for Universal Image Retrieval[[paper]](https://arxiv.org/pdf/2409.13513)[[code]](https://github.com/morrisfl/UniFEx)
8. [NeurIPS 2024 ]EMVP: Embracing Visual Foundation Model for Visual Place Recognition with Centroid-Free Probing[[paper]](https://openreview.net/pdf?id=V6w7keoTqn)[[code]](https://github.com/vincentqqb/EMVP)
9. [NeurIPS 2024 ]SuperVLAD: Compact and Robust Image Descriptors for Visual Place Recognition[[paper]](https://openreview.net/pdf?id=bZpZMdY1sj)[[code]](https://github.com/lu-feng/SuperVLAD)
   
 **vpr just change loss funtion:**  
1.[ICLR 2025]CLASS-RELATIONAL LABEL SMOOTHING FOR LIFELONG VISUAL PLACE RECOGNITION[[paper]](https://openreview.net/pdf?id=ZS1lCBLljq)[[code]](https://github.com/lu-feng/SuperVLAD)

**一个视觉场景的表征可以看成多个Part的聚合:**  
1.[TPAMI 2024]Unsupervised Part Discovery via Dual Representation Alignment[[paper]](https://arxiv.org/pdf/2408.08108)

**在测试时对特征进行矫正/About test time:**  
1. [ICLR 2025]TEST-TIME ADAPTATION FOR CROSS-MODAL RETRIEVAL WITH QUERY SHIFT[[paper]](https://arxiv.org/pdf/2410.15624)
2. [ICLR 2025]TEST-TIME ENSEMBLE VIA LINEAR MODE CONNECTIVITY: A PATH TO BETTER ADAPTATION[[paper]](https://openreview.net/pdfid=4wk2eOKGvh)
3. [NeurIPS 2024]BendVLM: Test-Time Debiasing of Vision-Language Embeddings[[paper]](https://arxiv.org/pdf/2411.04420) [[code]](https://github.com/waltergerych/bend_vlm)
4. [NeurIPS 2024]L-TTA: Lightweight Test-Time Adaptation Using a Versatile Stem Layer[[paper]](https://openreview.net/pdf?id=G7NZljVOol)[[code]](https://github.com/janus103/L_TTA)
5. [NeurIPS 2024]BoostAdapter: Improving Vision-Language Test-Time Adaptation via Regional Bootstrapping[[paper]](https://arxiv.org/pdf/2410.15430v2)[[code]](https://github.com/taolinzhang/BoostAdapter)
6. [NeurIPS 2024]Frustratingly Easy Test-Time Adaptation of Vision-Language Models[[paper]](https://arxiv.org/pdf/2405.18330)[[code]](https://github.com/FarinaMatteo/zero)
7. [NeurIPS 2024]Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach[[paper]](https://arxiv.org/pdf/2408.07511)[[code]](https://github.com/yarinbar/poem)
8. [NeurIPS 2024]Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models[[papr]](https://arxiv.org/pdf/2410.12790)[[code]](https://github.com/zhangce01/DPE-CLIP)
9. [CVPR 2024]Efficient Test-Time Adaptation of Vision-Language Models[[paper]](https://openaccess.thecvf.com/content/CVPR2024/papers/Karmanov_Efficient_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2024_paper.pdf)[[code]](https://kdiaaa.github.io/tda/)

**细粒度识别：**
1. [ECCV 2024]PDiscoFormer: Relaxing Part Discovery Constraints with Vision Transformers[[paper]](https://arxiv.org/pdf/2407.04538)[[code]](https://github.com/ananthu-aniraj/pdiscoformer)
2. [ICCV 2023]PDiscoNet: Semantically consistent part discovery for fine-grained recognition[[paper]](https://openaccess.thecvf.com/content/ICCV2023/papers/van_der_Klis_PDiscoNet_Semantically_consistent_part_discovery_for_fine-grained_recognition_ICCV_2023_paper.pdf)[[code]](https://github.com/robertdvdk/part_detection)


**监督对比学习：**
1. [NeurIPS-2020]Supervised Contrastive Learning[[paper]](https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf)[[code]](https://t.ly/supcon)

**图像配准：**
1. [CVPR-2024]Correspondence-Free Non-Rigid Point Set Registration Using
Unsupervised Clustering Analysis[[paper]](https://arxiv.org/pdf/2406.18817)[[code]](https://github.com/zikai1/CVPR24_PointSetReg)

   

      

      

---
**更新日期：2025年3月24**
